{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing for Tensorflow 2.x\n",
    "\n",
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('tenosrflow', '2.2.0', 'tokenizer', '0.2.0')"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import neologdn\n",
    "import tensorflow as tf\n",
    "\n",
    "import meguru_tokenizer\n",
    "from meguru_tokenizer.vocab import Vocab\n",
    "from meguru_tokenizer.whitespace_tokenizer import WhitespaceTokenizer\n",
    "from meguru_tokenizer.sentencepiece_tokenizer import SentencePieceTokenizer\n",
    "from meguru_tokenizer.sudachi_tokenizer import SudachiTokenizer\n",
    "\n",
    "\"tenosrflow\", tf.__version__, \"tokenizer\", meguru_tokenizer.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitespace Tokenizer for English, German, whose sentence is splitted by space.\n",
    "\n",
    "e.g. \"word word word bra bra bra\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Hello, I don't know how to use it?\",\n",
    "    \"Tensorflow is awesome!\",\n",
    "    \"it is good framework.\",\n",
    "]\n",
    "tokenizer = WhitespaceTokenizer(lower=True, language=\"en\")\n",
    "vocab = Vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building vocaburary\n",
    "\n",
    "min_freq means vocaburary will contain the words whose frequency >= min_freq\n",
    "\n",
    "vocab_size vocaburary will vocaburary size <= vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    vocab.add_vocabs(tokenizer.tokenize(sentence))\n",
    "vocab.build_vocab(min_freq=None, vocab_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab= vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dump and load vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<pad>\t0\n<s>\t1\n</s>\t2\n<unk>\t3\n<mask>\t4\nit\t5\nis\t6\nhello\t7\n,\t8\ni\t9\ndo\t10\nn't\t11\nknow\t12\nhow\t13\nto\t14\nuse\t15\n?\t16\ntensorflow\t17\nawesome\t18\n!\t19\ngood\t20\nframework\t21\n.\t22\n"
    }
   ],
   "source": [
    "vocab.dump_vocab(Path(\"vocab.txt\"))\n",
    "!cat vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab()\n",
    "vocab.load_vocab(Path(\"vocab.txt\"))\n",
    "tokenizer.vocab = vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['hello', ',', 'i', 'do', \"n't\", 'know', 'how', 'to', 'use', 'it', '?'],\n ['tensorflow', 'is', 'awesome', '!'],\n ['it', 'is', 'good', 'framework', '.']]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "tokenizer.tokenize_list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Hello, I don't know how to use it? -> [7, 8, 9, 10, 11, 12, 13, 14, 15, 5, 16] -> hello , i do n't know how to use it ?\nTensorflow is awesome! -> [17, 6, 18, 19] -> tensorflow is awesome !\nit is good framework. -> [5, 6, 20, 21, 22] -> it is good framework .\n"
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    enc = tokenizer.encode(sentence)\n",
    "    dec = tokenizer.decode(enc)\n",
    "    print(f'{sentence} -> {enc} -> {dec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode text for tensorflow's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<TextLineDatasetV2 shapes: (), types: tf.string>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "with Path(\"source.txt\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(sentence + \"\\n\")\n",
    "\n",
    "ds = tf.data.TextLineDataset(\"source.txt\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<PrefetchDataset shapes: (None, None), types: tf.int64>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "bos_id = tokenizer.vocab.word2idx(tokenizer.vocab.bos)\n",
    "\n",
    "def encode(text):\n",
    "    encoded_text = tokenizer.encode(text.numpy().decode())\n",
    "    return np.concatenate(([bos_id], encoded_text))\n",
    "\n",
    "def encoded_map_fn(text):\n",
    "    encoded_text = tf.py_function(encode, inp=[text], Tout=(tf.int64))\n",
    "    encoded_text.set_shape([None])\n",
    "    return encoded_text\n",
    "\n",
    "encoded_ds = ds.map(encoded_map_fn).shuffle(buffer_size=len(sentences)).padded_batch(3).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "encoded_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch : 0\n[padded batch]\ntf.Tensor(\n[[ 1  5  6 20 21 22  0  0  0  0  0  0]\n [ 1 17  6 18 19  0  0  0  0  0  0  0]\n [ 1  7  8  9 10 11 12 13 14 15  5 16]], shape=(3, 12), dtype=int64)\nepoch : 1\n[padded batch]\ntf.Tensor(\n[[ 1  7  8  9 10 11 12 13 14 15  5 16]\n [ 1 17  6 18 19  0  0  0  0  0  0  0]\n [ 1  5  6 20 21 22  0  0  0  0  0  0]], shape=(3, 12), dtype=int64)\nepoch : 2\n[padded batch]\ntf.Tensor(\n[[ 1  5  6 20 21 22  0  0  0  0  0  0]\n [ 1  7  8  9 10 11 12 13 14 15  5 16]\n [ 1 17  6 18 19  0  0  0  0  0  0  0]], shape=(3, 12), dtype=int64)\n"
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    print(\"epoch :\", epoch)\n",
    "    for batch in encoded_ds:\n",
    "        print(\"[padded batch]\")\n",
    "        print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentencepiece Tokenizer for Any language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<pad>\t0\n<s>\t0\n</s>\t0\n<unk>\t0\n<mask>\t0\n▁\t-1.85354\no\t-2.41476\nw\t-2.69918\ns\t-2.92457\ne\t-2.94918\nn\t-3.28251\nt\t-3.74824\n▁i\t-3.74824\nd\t-3.78251\na\t-3.78251\nf\t-3.78251\nme\t-3.78251\nk\t-3.78251\n▁it\t-3.81797\nlo\t-3.83735\nor\t-4.00886\nr\t-4.35741\nl\t-4.65398\n!\t-4.78251\n'\t-4.78251\n,\t-4.78251\n.\t-4.78251\n?\t-4.78251\nH\t-4.78251\nI\t-4.78251\ng\t-4.78251\nh\t-4.78251\nu\t-4.78251\nT\t-4.78251\nm\t-4.93682\ni\t-4.93692\nso\t-4.93692\n"
    }
   ],
   "source": [
    "tokenizer = SentencePieceTokenizer(lower=True, language=\"en\")\n",
    "sentences = [\n",
    "    \"Hello, I don't know how to use it?\",\n",
    "    \"Tensorflow is awesome!\",\n",
    "    \"it is good framework.\",\n",
    "]\n",
    "\n",
    "source_file = Path(\"source.txt\")\n",
    "with source_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for s in sentences:\n",
    "        f.write(s + \"\\n\")\n",
    "\n",
    "tokenizer.train_sp(source_file, vocab_size=37)\n",
    "\n",
    "!cat m.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['▁', 'h', 'e', 'l', 'lo', ',', '▁i', '▁', 'd', 'o', 'n', \"'\", 't', '▁', 'k', 'n', 'o', 'w', '▁', 'h', 'o', 'w', '▁', 't', 'o', '▁', 'u', 's', 'e', '▁it', '?']\n['▁', 't', 'e', 'n', 's', 'or', 'f', 'lo', 'w', '▁i', 's', '▁', 'a', 'w', 'e', 'so', 'me', '!']\n['▁it', '▁i', 's', '▁', 'g', 'o', 'o', 'd', '▁', 'f', 'r', 'a', 'me', 'w', 'or', 'k', '.']\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<TextLineDatasetV2 shapes: (), types: tf.string>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "bos_id = tokenizer.vocab.word2idx(tokenizer.vocab.bos)\n",
    "\n",
    "with Path(\"source.txt\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(sentence + \"\\n\")\n",
    "        print(tokenizer.tokenize(sentence))\n",
    "\n",
    "ds = tf.data.TextLineDataset(\"source.txt\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<PrefetchDataset shapes: (None, None), types: tf.int64>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "def encode(text):\n",
    "    encoded_text = tokenizer.encode(text.numpy().decode())\n",
    "    return np.concatenate(([bos_id], encoded_text))\n",
    "\n",
    "def encoded_map_fn(text):\n",
    "    encoded_text = tf.py_function(encode, inp=[text], Tout=(tf.int64))\n",
    "    encoded_text.set_shape([None])\n",
    "    return encoded_text\n",
    "\n",
    "encoded_ds = ds.map(encoded_map_fn).shuffle(buffer_size=len(sentences)).padded_batch(3).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "encoded_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch : 0\n[padded batch]\ntf.Tensor(\n[[ 1  5 11  9 10  8 20 15 19  7 12  8  5 14  7  9 36 16 23  0  0  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 1 18 12  8  5 30  6  6 13  5 15 21 14 16  7 20 17 26  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 1  5 31  9 22 19 25 12  5 13  6 10 24 11  5 17 10  6  7  5 31  6  7  5\n  11  6  5 32  8  9 18 27]], shape=(3, 32), dtype=int64)\nepoch : 1\n[padded batch]\ntf.Tensor(\n[[ 1  5 11  9 10  8 20 15 19  7 12  8  5 14  7  9 36 16 23  0  0  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 1 18 12  8  5 30  6  6 13  5 15 21 14 16  7 20 17 26  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 1  5 31  9 22 19 25 12  5 13  6 10 24 11  5 17 10  6  7  5 31  6  7  5\n  11  6  5 32  8  9 18 27]], shape=(3, 32), dtype=int64)\nepoch : 2\n[padded batch]\ntf.Tensor(\n[[ 1 18 12  8  5 30  6  6 13  5 15 21 14 16  7 20 17 26  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0]\n [ 1  5 31  9 22 19 25 12  5 13  6 10 24 11  5 17 10  6  7  5 31  6  7  5\n  11  6  5 32  8  9 18 27]\n [ 1  5 11  9 10  8 20 15 19  7 12  8  5 14  7  9 36 16 23  0  0  0  0  0\n   0  0  0  0  0  0  0  0]], shape=(3, 32), dtype=int64)\n"
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    print(\"epoch :\", epoch)\n",
    "    for batch in encoded_ds:\n",
    "        print(\"[padded batch]\")\n",
    "        print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sudachi Tokenizer for Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "23"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "sentences = [\"銀座でランチをご一緒しましょう。\", \"締切間に合いますか？\", \"トークナイザを作りました。\"]\n",
    "\n",
    "tokenizer = SudachiTokenizer(language=\"ja\")\n",
    "vocab = Vocab()\n",
    "\n",
    "for sentence in sentences:\n",
    "    vocab.add_vocabs(tokenizer.tokenize(sentence))\n",
    "vocab.build_vocab()\n",
    "\n",
    "tokenizer.vocab = vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('銀座', 'で', 'ランチ', 'を', 'ご', '一緒', 'し', 'ましょう', '。')\n('締切', '間に合い', 'ます', 'か', '?')\n('トークナイザ', 'を', '作り', 'まし', 'た', '。')\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<TextLineDatasetV2 shapes: (), types: tf.string>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "bos_id = tokenizer.vocab.word2idx(tokenizer.vocab.bos)\n",
    "\n",
    "with Path(\"source.txt\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(sentence + \"\\n\")\n",
    "        print(tokenizer.tokenize(sentence))\n",
    "\n",
    "ds = tf.data.TextLineDataset(\"source.txt\")\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<PrefetchDataset shapes: (None, None), types: tf.int64>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "def encode(text):\n",
    "    encoded_text = tokenizer.encode(text.numpy().decode())\n",
    "    return np.concatenate(([bos_id], encoded_text))\n",
    "\n",
    "def encoded_map_fn(text):\n",
    "    encoded_text = tf.py_function(encode, inp=[text], Tout=(tf.int64))\n",
    "    encoded_text.set_shape([None])\n",
    "    return encoded_text\n",
    "\n",
    "encoded_ds = ds.map(encoded_map_fn).shuffle(buffer_size=len(sentences)).padded_batch(3).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "encoded_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch : 0\n[padded batch]\ntf.Tensor(\n[[ 1 19  5 20 21 22  6  0  0  0]\n [ 1  7  8  9  5 10 11 12 13  6]\n [ 1 14 15 16 17 18  0  0  0  0]], shape=(3, 10), dtype=int64)\nepoch : 1\n[padded batch]\ntf.Tensor(\n[[ 1  7  8  9  5 10 11 12 13  6]\n [ 1 19  5 20 21 22  6  0  0  0]\n [ 1 14 15 16 17 18  0  0  0  0]], shape=(3, 10), dtype=int64)\nepoch : 2\n[padded batch]\ntf.Tensor(\n[[ 1  7  8  9  5 10 11 12 13  6]\n [ 1 19  5 20 21 22  6  0  0  0]\n [ 1 14 15 16 17 18  0  0  0  0]], shape=(3, 10), dtype=int64)\n"
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    print(\"epoch :\", epoch)\n",
    "    for batch in encoded_ds:\n",
    "        print(\"[padded batch]\")\n",
    "        print(batch)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594651077155",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}